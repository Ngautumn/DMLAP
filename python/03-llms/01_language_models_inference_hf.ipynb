{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "## Language Models | Inference (Huggingface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "For more, see [here](https://huggingface.co/tasks/text-generation) and [here](https://huggingface.co/docs/transformers/generation_strategies).\n",
    "\n",
    "Notebook heavily based on Jérémie Wenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "## Install & Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "#### Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "\n",
    "import os\n",
    "os.chdir(\"drive/My Drive/gold/DMLAP\") # to change to another directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "#### Huggingface login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "For some models and datasets, and if you want to push your model to HF (same as GitHub, but for models) you need to be logged into your HF account.\n",
    "\n",
    "For that, you need to create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "```python\n",
    "import pathlib\n",
    "from huggingface_hub import notebook_login\n",
    "if not (pathlib.Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "URjvsuUyIthY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Shuran Zhang\\AppData\\Local\\Temp\\ipykernel_1988\\3167684449.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "d:\\Miniforge3\\envs\\dmlap\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    # note: models using bfloat16 aren't compatible with MPS\n",
    "    # else \"mps\"\n",
    "    # if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "from transformers import pipeline               # pipelines are high-level APIs that allow you to perform various NLP tasks with just a few lines of code, such as text generation, sentiment analysis, etc.\n",
    "from transformers import GenerationConfig       # class that holds all the parameters for text generation, such as temperature, top_k, top_p, etc.\n",
    "\n",
    "from transformers import AutoTokenizer          # tokenizers are used to convert text into tokens (numbers) that the model can understand\n",
    "from transformers import AutoModelForCausalLM   # text generation models also called causal language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The textwrap module automatically formats text for you\n",
    "import textwrap\n",
    "\n",
    "# many more options, see them with textwrap.TextWrapper?\n",
    "tw = textwrap.TextWrapper(\n",
    "    # the formatted width we want\n",
    "    width=79,\n",
    "    # this will keep whitespace & line breaks in the original text\n",
    "    replace_whitespace=False\n",
    ")\n",
    "\n",
    "def wrap_print(s):\n",
    "    \"\"\"Format text into Textwrapped lines and print it\"\"\"\n",
    "    print(\"\\n\".join(tw.wrap(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-the-box Generation: the `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic object in the Transformers library is the `pipeline` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n",
    "\n",
    "The `pipeline` works for literally everything, so we need to specify the task (`text-generation`), and the model ([so many to choose from...](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads), here's [the current model page](https://huggingface.co/Qwen/Qwen3-0.6B?library=transformers)). We also select the [device](https://huggingface.co/docs/transformers/pipeline_tutorial#device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Tf3fn2Z_DV"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenerationConfig is a container of the generation hyperparameters (e.g. max_length, temperature, top_k, top_p, do_sample, pad_token_id, etc.). \n",
    "\n",
    "See [here](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.example) for an example using `GenerationConfig` and [here](https://github.com/huggingface/transformers/issues/19853#issuecomment-1290759818) for the `pad_token_id` fix.\n",
    "\n",
    "Below, we load the parameters from the pre-trained model of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sueT3i6MfM8d"
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config.max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to edit a hyperparameter, rather than using the ones that come with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fZaJdUnPg7E"
   },
   "outputs": [],
   "source": [
    "generation_config.max_length = 25        # the maximum length of the generated text (including the input prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to edit more. If so, print out the updated generation_config, to compare with the one earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_config.temperature = 0.7      # lower = more deterministic, higher = more random\n",
    "# generation_config.do_sample = True       # enables sampling | if False, disables sampling, uses greedy search\n",
    "# generation_config.top_k = 50             # limits sampling to top 50 tokens\n",
    "# generation_config.top_p = 0.95           # nucleus sampling (probability mass)\n",
    "\n",
    "# print(generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW2UOpO2QAFD"
   },
   "source": [
    "### Quick vocab note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW2UOpO2QAFD"
   },
   "source": [
    "`bos`: beginning of sentence  \n",
    "`eos`: end of sentence  \n",
    "`pad`: padding\n",
    "\n",
    "These are special tokens that have been inserted into the text at training time.\n",
    "\n",
    "For instance, in the case below, the 'beginning' of the text is `<|endoftext>|`, as during training the texts are fed to the network one after the other, with this special token between them.\n",
    "\n",
    "If you load a different model and check its `generation_config.bos_token_id` and decode it, you may see a different string, eg a `<BOS>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number (token id) representing the beginning of sentence special token\n",
    "bos_token_id = generation_config.bos_token_id\n",
    "print(bos_token_id)\n",
    "# decode that number back into a string\n",
    "print(generator.tokenizer.decode([bos_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61kgReONYBT5",
    "outputId": "cd2bae9a-2feb-44d7-c977-e371fea14c38"
   },
   "outputs": [],
   "source": [
    "generator(\n",
    "    \"Once upon a time,\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McOEE98ITaoz"
   },
   "source": [
    "Parallel generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40-Zwu0ETWnW",
    "outputId": "34296884-51a8-46ff-baba-9ac0e1a8ff97"
   },
   "outputs": [],
   "source": [
    "generator(\n",
    "    [\"Once upon a time,\"] * 2,\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "## Deeper approach: `Tokenizer` and `Model` classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "What does the `pipeline` function do under the hood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EzS8SV5ccuu"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# to GPU/MPS/CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXJjd_kEREkS"
   },
   "source": [
    "### The tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXJjd_kEREkS"
   },
   "source": [
    "See [the Preprocess](https://huggingface.co/docs/transformers/main/en/preprocessing) tutorial on Huggingface for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "# you can just pass a string, and see the full object the model expects\n",
    "toks = tokenizer(\"Oh sweet midnight\")\n",
    "print(toks)\n",
    "\n",
    "# the attention mask is used to differentiate between regular tokens and padding tokens\n",
    "# here, since there is no padding, all tokens are regular tokens, so the attention mask is all 1s\n",
    "print(tokenizer.decode(toks[\"input_ids\"]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "# or use encode, and get only the tokens (not the attention mask)\n",
    "toks = tokenizer.encode(\"Oh sweet midnight\")\n",
    "print(toks)\n",
    "\n",
    "# this is now just a tensor, not a dictionary with input_ids and attention_mask\n",
    "print(tokenizer.decode(toks))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "# encode multiple sequences\n",
    "toks = tokenizer([\"Oh sweet midnight\", \"harbinger of doom\"])\n",
    "print(toks)\n",
    "print(tokenizer.batch_decode(toks[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above, the number of input_ids is the same as the number of words. But this is not always the case, depending on the input words. In the example below, you will notice how tokens often correspond to subwords. There are also certain subword markers (e.g. Ġ, ##) are tokenizer-specific and indicate word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer(\"harbinger of doom\")\n",
    "ids = toks[\"input_ids\"]\n",
    "print(\"ids:\", ids)\n",
    "\n",
    "tokens = tokenizer.tokenize(\"harbinger of doom\")\n",
    "print(\"tokens:\", tokens)\n",
    "\n",
    "tokens_from_ids = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(tokens_from_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "# add padding\n",
    "toks = tokenizer([\"Oh sweet midnight\", \"harbinger of doom\"], padding=True)\n",
    "\n",
    "print(\"padding token:\", tokenizer.encode(tokenizer.special_tokens_map['pad_token']))\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "# here we see the padding in the decoding\n",
    "print(tokenizer.batch_decode(toks[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use `skip_special_tokens` to remove that\n",
    "print(tokenizer.batch_decode(toks[\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "# for this example, let's pretend our max length is 2\n",
    "max_len_orig = tokenizer.model_max_length\n",
    "tokenizer.model_max_length = 2\n",
    "\n",
    "# in some cases, you also need truncation\n",
    "toks = tokenizer([\"Oh sweet midnight\", \"harbinger of doom\"], padding=True, truncation=True)\n",
    "\n",
    "print(\"padding token:\", tokenizer.encode(tokenizer.special_tokens_map['pad_token']))\n",
    "print(toks)\n",
    "print(tokenizer.batch_decode(toks[\"input_ids\"]))\n",
    "\n",
    "# restore the tokenizer's accurate max length\n",
    "tokenizer.model_max_length = max_len_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gju_L3gKVN4j",
    "outputId": "3914d075-337b-4850-a223-6fb6ac0cf7ad"
   },
   "outputs": [],
   "source": [
    "# small example of manipulating the tokens manually\n",
    "# # return_tensors=\"pt\" returns a PyTorch tensor instead of a list of token ids\n",
    "input_ids = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "\n",
    "# just copying the tensor 4 times\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(device)\n",
    "print(batched_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVKQUtN9eEpW"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on, return pytorch tensors\n",
    "input_ids = tokenizer(\n",
    "    [\"Once upon a time\"] * 4,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "# same logic as before\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "generation_config.max_length = 100\n",
    "# suppressing a pesky warning (https://stackoverflow.com/a/71397707)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 100\n",
    "output = model.generate(\n",
    "    **input_ids,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to edit the generation config to use sampling instead of greedy decoding, and adjust the temperature, top_k or top_p parameters:\n",
    "\n",
    "# generation_config.temperature = 0.7      # lower = more deterministic, higher = more random\n",
    "# generation_config.do_sample = True       # enables sampling (not greedy)\n",
    "# generation_config.top_k = 50             # limits sampling to top 50 tokens\n",
    "# generation_config.top_p = 0.95           # nucleus sampling (probability mass)\n",
    "\n",
    "# output = model.generate(\n",
    "#     **input_ids,\n",
    "#     generation_config=generation_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibq2IrdhhSAH",
    "outputId": "4f4d119b-a32b-440a-d3ec-144a83d48e1f"
   },
   "outputs": [],
   "source": [
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "for t in texts:\n",
    "    wrap_print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat models & templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly recommended to get an intuition: [tiktokenizer, an interactive tokenizer web app](https://tiktokenizer.vercel.app/), further explaines [here](https://www.listedai.co/ai/tiktokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With the `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Chat basics page](https://huggingface.co/docs/transformers/en/conversations).\n",
    "\n",
    "Chat models accept a list of messages (the chat history) as the input. Each message is a dictionary with role and content keys. To start the chat, add a single user message. You can also optionally include a system message to give the model directions on how to behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful science assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you define gravity in one sentence?\"}\n",
    "]\n",
    "\n",
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "response = generator(chat, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0]: only one batch\n",
    "# inside that, [\"generated_text\"] contains the whole chat\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the model’s response, you can continue the conversation by appending a new user message to the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the whole chat\n",
    "chat = response[0][\"generated_text\"]\n",
    "# add a new user response\n",
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"Woah! But can it be reconciled with quantum mechanics?\"}\n",
    ")\n",
    "\n",
    "# generate again\n",
    "response = generator(chat, max_new_tokens=512)\n",
    "\n",
    "# print the response\n",
    "print(response[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual tokenization & formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Chat template page](https://huggingface.co/docs/transformers/en/chat_templating).\n",
    "\n",
    "In order to make LLMs behave like chatbots is by fine-tuning them on text that follows a specific format, with markers (called \"special tokens\") that won't necessarily be seen by the user, but can be detected internally, and signify \"this is the bot speaking\", \"the reply ends here\", etc. By detecting these particular tokens (remember that they are just numbers under the hood), it is possible e.g. to stop generation whenever the special token for the end-of-reply is detected. \n",
    "\n",
    "Formatting text in this way is called \"templating\", and the scheme for such a thing is a \"chat template\". Each model has its own, even if they are mostly similar. The template for our model can be found [here](https://ollama.com/library/qwen3:0.6b/blobs/ae370d884f10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying the full template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to give the model the right format (models behave best when the input is formatted in the same way as the data they have been trained on), we use `tokenizer.apply_chat_template`. Notice how having `add_generation_prompt=True` adds this at the end of our chat:\n",
    "```\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "This changes the text the model reads, and since the only thing it does is continue the text to the best of its abilities, now it will continue in it by generating what the 'assistant' would say.\n",
    "\n",
    "Note also the 'thinking' bit: 'disable thinking' actually just means adding:\n",
    "```\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "```\n",
    "to the text, which means the model will just continue generating what should come after an empty thinking block..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "    tokenize=True,\n",
    "    # return_dict=True,\n",
    "\tadd_generation_prompt=True,\n",
    "\treturn_tensors=\"pt\",\n",
    "    # if True, the model will write preliminary steps in a <think> block before answering\n",
    "    enable_thinking=False\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(tokenized_chat[\"input_ids\"][0])) # if you return a dictionary, you need to access the input_ids key\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve only the response, you would have to slice manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids=tokenized_chat, max_new_tokens=100)\n",
    "\n",
    "input_length = tokenized_chat.shape[1]\n",
    "answer_tokens = outputs[0][input_length:]  # slice the output tensor\n",
    "print(tokenizer.decode(answer_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Leaving the template incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we dont have `add_generation_prompt`, the tokenizer does not add the directing bit at the end:\n",
    "```\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat_no_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(tokenized_chat_no_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your use case, you might want that. The model could now continue in a different way than adding an assistant response..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unclosed message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even go further, and not even have the special 'end' token, meaning that in all likelihood the model will continue the last message. This can be useful if you wanted to put words in the model's mouth (prefilling its response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclosed_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    continue_final_message=True # the model will continue the last message instead of adding a new generation prompt, so you can have an \"unclosed\" chat where the model is expected to continue the last message\n",
    ").to(device)\n",
    "\n",
    "print(tokenizer.decode(unclosed_chat[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclosed_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**unclosed_chat, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "## To do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "1. Test everything! Make sure you understand and develop an intuition of:\n",
    " - The various parameters: `temperature`, `top_k`, `top_p`\n",
    " - The `tokenizer` object to convert text into tokens and back\n",
    " - How to handle the whole pipeline\n",
    "   Also, you can search for different [models](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)! (Some of them may exceed your GPU capacity, beware). People have finetuned language models on many types of texts.\n",
    "2. Can you think of a way to introduce computational thinking into this? Ideas:\n",
    "  - First, you could explore ways of making things look nicer? Instead of just having a list of objects? You could write a nice print function that knows exactly how to take the model output and print it in a nice way. The specialised Python package with many text functionalities is [textwrap](https://docs.python.org/3/library/textwrap.html) (see also [here](https://www.geeksforgeeks.org/textwrap-text-wrapping-filling-python/));\n",
    "  - Can you think of ways to construct a writing **loop**? By that, I mean:  \n",
    "    a. Prepare prompt  \n",
    "    b. Generate one or more strands of text  \n",
    "    c. Select text from strands, go back to a.  \n",
    "    This could simply mean writing a system of helper functions and classes to assist you in the writing...\n",
    "  - One could imagine all sorts of strange ways to work with text, from programmatically chunking the generated text and scrambling it before using it again as a prompt, to explore what the model does if you use unreasonable parameters (e.g. a very high or low `temperature`).\n",
    "  - Also, can you think of ways to work with various strands of text (Taking advantage of the fact that a model can generate in parallel)?\n",
    "\n",
    "3. Something that has already been the subject of a lot of debate and controversy, is the exploration of the *biases* of the models (and there are tons!). LLMs are trained mostly on Internet text, top-ranked reddit posts, etc. (see for instance [this open-source replication](https://github.com/jcpeterson/openwebtext)). Unsurprisingly, the topics and points of view reflect that corner of human activities... Differences between American and Chinese models are also interesting to explore."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "dmlap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
